<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pradeep Pujari | Data Engineer</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Pradeep Pujari</h1>
            <p>Data Engineer | 3+ Years of Experience</p>
            <nav>
                <a href="#about">About</a>
                <a href="#skills">Skills</a>
                <a href="#experience">Experience</a>
                <a href="#projects">Projects</a>
                <a href="#education">Education</a>
                <a href="#contact">Contact</a>
            </nav>
        </div>
    </header>
    <main>
        <section id="about">
            <h2>Career Summary</h2>
            <p>
                Results-driven Data Engineer with over 3+ years of experience in designing, implementing, and optimizing data pipelines and ETL processes. Specialize in harnessing Big Data tools and cloud platforms. Passionate about data engineering and contributing to impactful data-driven strategies.
            </p>
        </section>
        <section id="skills">
            <h2>Technical Skills</h2>
            <ul>
                <li><strong>Programming Languages:</strong> Python, Scala, SQL, PySpark, SparkSQL</li>
                <li><strong>Big Data Technologies:</strong> Hadoop, Spark, AWS EMR</li>
                <li><strong>Databases & OS:</strong> MySQL, Oracle, HBase, Snowflake, Windows, Linux</li>
                <li><strong>Cloud Platforms:</strong> AWS and Its Services</li>
                <li><strong>Data Warehousing:</strong> Hive, HBase, Amazon S3, Impala, Hue, AWS Athena</li>
                <li><strong>Scheduling Tools:</strong> AWS Glue, Sqoop, Cron Jobs</li>
                <li><strong>Version Control:</strong> Git, GitHub, Jira</li>
            </ul>
        </section>
        <section id="experience">
            <h2>Work Experience</h2>
            <h3>Data Engineer, Hitachi Vantara</h3>
            <p><em>May 2021 â€“ Present (Remote, Pune)</em></p>
            <ul>
                <li>Developed scalable ETL pipelines with Hadoop, Spark, and AWS.</li>
                <li>Integrated diverse data sources for seamless data flow.</li>
                <li>Optimized data transformations in Hive, Spark, and AWS for improved performance.</li>
                <li>Collaborated across teams to deliver timely solutions.</li>
                <li>Migrated workflows to AWS, leveraging Glue, Athena, S3, and EMR clusters.</li>
                <li>Automated ETL processes and scheduled data jobs.</li>
                <li>Implemented CI/CD practices for seamless pipeline deployment.</li>
                <li>Reduced ETL runtime by 35% through Spark optimization techniques.</li>
            </ul>
        </section>
        <section id="projects">
            <h2>Projects</h2>
            <h3>Financial Intelligence Unit</h3>
            <p>Designed an ETL pipeline for processing credit card transactions to determine rewards, EMI eligibility, and discounts.</p>
            <ul>
                <li><strong>Tech Stack:</strong> Spark, Hive, Python, Scala, AWS S3, EMR, Athena, Glue</li>
                <li><strong>Workflow:</strong>
                    <ul>
                        <li>Ingested and cataloged transactional data using Hive and AWS Glue Crawlers.</li>
                        <li>Processed data using PySpark and Scala Spark for cleaning, transformation, and aggregation.</li>
                        <li>Stored data in Hive Tables and AWS S3 in Parquet format for efficient querying.</li>
                        <li>Enabled querying and analysis with Athena and Impala.</li>
                    </ul>
                </li>
            </ul>
            <h3>Data Migration and Integration</h3>
            <p>Developed an ETL pipeline for migrating and integrating data into a centralized repository using on-premises data computing capabilities.</p>
            <ul>
                <li><strong>Tech Stack:</strong> SQL, Python, Scala, PySpark, Sqoop, Hive, HBase, Cron Jobs</li>
                <li><strong>Workflow:</strong>
                    <ul>
                        <li>Imported data from RDBMS to HDFS using Sqoop.</li>
                        <li>Transformed data with PySpark and Hive optimization techniques.</li>
                        <li>Stored data in HBase for low-latency access.</li>
                        <li>Orchestrated workflows using Cron Jobs and CI/CD pipelines.</li>
                    </ul>
                </li>
            </ul>
        </section>
        <section id="education">
            <h2>Education</h2>
            <p><strong>Bachelor of Engineering:</strong> Pune University, Pune</p>
            <p><strong>HSC Computer Science:</strong> MSBHSC, Pune</p>
        </section>
        <section id="contact">
            <h2>Contact Me</h2>
            <p><strong>Email:</strong> <a href="mailto:pradeep.m.pujari@gmail.com">pradeep.m.pujari@gmail.com</a></p>
            <p><strong>Phone:</strong> +91 8208022944</p>
            <p><strong>LinkedIn:</strong> <a href="https://linkedin.com/in/pradeep-pujari-736b8a27a/" target="_blank">linkedin.com/in/pradeep-pujari</a></p>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Pradeep Pujari. All Rights Reserved.</p>
    </footer>
</body>
</html>
